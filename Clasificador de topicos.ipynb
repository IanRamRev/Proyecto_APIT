{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Los topicos se encuentran juntos en el dataframe, esta función los separa de acuerdo al tema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separaTopico(topico, df): #Separa todos los encabezados de acuerdo a su tópico\n",
    "    listTopic = []\n",
    "    dataArr = numpy.asarray(df)\n",
    "    for i in range(len(dataArr)):\n",
    "        if(dataArr[i][1] == topico):\n",
    "            listTopic.append(dataArr[i][0])\n",
    "    return listTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cuando se tienen las palabras unicas de cada tópico, se eliminan caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaTexto(text):\n",
    "    \"List all the word tokens in a text.\"\n",
    "    return re.findall('[a-zA-Z0-9\\-]{4,254}', text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cada encabezado corresponde a un documento, esta función separa cada palabra para agregarlas a un solo array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PalabrasTopico(listTopico): #Devuelve todas las palabras que aparecen en los titulares de las noticias \n",
    "    listHeadLines = []\n",
    "    for i in range(len(listTopico)):\n",
    "        #aux = nltk.word_tokenize(arrayTopico[i])\n",
    "        aux = limpiaTexto(listTopico[i])\n",
    "        for j in range(len(aux)):\n",
    "            aux2 = aux[j]\n",
    "            listHeadLines.append(aux2)\n",
    "            \n",
    "    return numpy.asarray(listHeadLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilidad(palabrasTopico, palabrasUnicasTopico): #Devuelve un array con la ocurrencias de cada única del tópico\n",
    "    Prob = numpy.zeros(len(palabrasUnicasTopico))\n",
    "    for i in range(len(palabrasUnicasTopico)):\n",
    "        Prob[i] = numpy.sum(palabrasTopico == palabrasUnicasTopico[i])\n",
    "        Prob[i] = Prob[i] /len(palabrasTopico)\n",
    "        \n",
    "    return Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recibe el vector de probabilidades y elige la máxima para finalmente devolver el tópico al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clasificador(arrayProbabilidades):\n",
    "    indice = numpy.where(arrayProbabilidades == max(arrayProbabilidades))[0][0]\n",
    "    print(\"Indice = \", type(indice))\n",
    "    print(\"Max = \", max(arrayProbabilidades))\n",
    "    if (indice == 0):\n",
    "        print(\"Covid\")\n",
    "    if (indice == 1):\n",
    "        print(\"Tecnología\")\n",
    "    if (indice == 2):\n",
    "        print(\"Deportes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep):\n",
    "    T = nltk.word_tokenize(tweet)\n",
    "    Proba = numpy.ones(3)\n",
    "\n",
    "    #Covid\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCovid == T[j]) + 1\n",
    "        Proba[0] *= (aux + 1) / (len(AlfabetoCovid) + aux)\n",
    "    \n",
    "    #Tecnologia\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasTecn == T[j]) \n",
    "        Proba[1] *= (aux + 1) / (len(AlfabetoTecn) + aux)\n",
    "\n",
    "    #Deportes\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasDep == T[j]) + 1\n",
    "        Proba[2] *= (aux + 1) / (len(AlfabetoDep) + aux)\n",
    "\n",
    "    for p in range(len(Proba)):\n",
    "        Proba[p] = Proba[p] * 1/3\n",
    "        \n",
    "    return Clasificador(Proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abrimos el dataset con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Tópico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que política o economía conduzcan respuesta al...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oaxaca habilita red de hospitales para covid-19</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgen más brotes de covid-19 en asilo y empre...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suman cuatro mil 477 muertos y 42 mil 595 cont...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Revelan que más de mil 400 mujeres han solicit...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>En los Parapanamericanos, México suma ya 42 or...</td>\n",
       "      <td>deportes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>El futbolista Jonathan Fabbro es condenado a 1...</td>\n",
       "      <td>deportes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>México cosecha 50 medallas en el paraatletismo...</td>\n",
       "      <td>deportes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Debut y gol de Edson Álvarez que da pase al Aj...</td>\n",
       "      <td>deportes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>Rosa Guerrero, oro en lanzamiento de disco en ...</td>\n",
       "      <td>deportes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Titular    Tópico\n",
       "0     Que política o economía conduzcan respuesta al...     covid\n",
       "1       Oaxaca habilita red de hospitales para covid-19     covid\n",
       "2     Surgen más brotes de covid-19 en asilo y empre...     covid\n",
       "3     Suman cuatro mil 477 muertos y 42 mil 595 cont...     covid\n",
       "4     Revelan que más de mil 400 mujeres han solicit...     covid\n",
       "...                                                 ...       ...\n",
       "1195  En los Parapanamericanos, México suma ya 42 or...  deportes\n",
       "1196  El futbolista Jonathan Fabbro es condenado a 1...  deportes\n",
       "1197  México cosecha 50 medallas en el paraatletismo...  deportes\n",
       "1198  Debut y gol de Edson Álvarez que da pase al Aj...  deportes\n",
       "1199  Rosa Guerrero, oro en lanzamiento de disco en ...  deportes\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = ['Titular', 'Tópico']\n",
    "dataTrain = pandas.read_csv('titulares.csv', names=nombres)\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separamos del dataset por tópicos y a cada encabezado lo separamos por palabra de los documentos eliminando caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1399\n",
      "['2018' '2019' '2021' '30-50' 'abacus' 'abierto' 'abre' 'abrir'\n",
      " 'aburridos' 'abusivas' 'acad' 'acciones' 'acepta' 'acoso' 'acotar'\n",
      " 'activista' 'acusa' 'acusaciones' 'acusan' 'afecta' 'afectados' 'agota'\n",
      " 'agradable' 'ahora' 'airpods' 'alan' 'alexa' 'algo' 'algoritmo'\n",
      " 'algoritmos' 'algunos' 'alista' 'alphabet' 'alternativa' 'alternativas'\n",
      " 'amazon' 'amenaza' 'amigo' 'amigos' 'amlo' 'analytica' 'ancestro'\n",
      " 'android' 'androide' 'anel' 'anonymous' 'ante' 'antig' 'antiguas'\n",
      " 'antiguos' 'antitabaco' 'anuncia' 'anuncian' 'anunciantes' 'aparecer'\n",
      " 'aplicaci' 'aplicaciones' 'aplicar' 'apocalipsis' 'apoyar' 'apple' 'apps'\n",
      " 'aprendi' 'apuesta' 'apunta' 'archivos' 'aretes' 'artificial'\n",
      " 'artificiales' 'arya' 'asesinato' 'asesino' 'asilo' 'asistente'\n",
      " 'assassin' 'ataca' 'audios' 'aumenta' 'aunque' 'autorretratos' 'auxilio'\n",
      " 'avatar' 'avecina' 'ayuda' 'baja' 'bajo' 'banner' 'barcelona' 'batalla'\n",
      " 'batallas' 'battle' 'bernie' 'bicicleta' 'bien' 'bill' 'billete'\n",
      " 'bitcoin' 'bitos' 'blackberry' 'blico' 'blicos' 'bloomberg' 'bloque'\n",
      " 'bloquea' 'bloquear' 'bluetooth' 'bola' 'borra' 'brica' 'bueno' 'burla'\n",
      " 'burlan' 'buscadas' 'buscan' 'buscar' 'byte' 'cada' 'cambi' 'cambiar'\n",
      " 'cambridge' 'caminar' 'campa' 'campo' 'cancelaci' 'cancelan' 'caramelo'\n",
      " 'carlos' 'casi' 'caso' 'castigan' 'caza' 'cdmx' 'ceden' 'celebridades'\n",
      " 'celular' 'censura' 'centennialls' 'cerdos' 'cerebro' 'certamen'\n",
      " 'charlas' 'china' 'chrome' 'ciberataques' 'ciberseguridad' 'ciencia'\n",
      " 'cientos' 'ciernen' 'cinco' 'cines' 'cita' 'citas' 'clic' 'click' 'club'\n",
      " 'cnicas' 'cnico' 'colectiva' 'colima' 'combatir' 'comercial'\n",
      " 'comerciales' 'como' 'comp' 'comparte' 'competidor' 'competir' 'compra'\n",
      " 'computaci' 'computadoras' 'conecta' 'conectada' 'conflictos' 'congreso'\n",
      " 'congress' 'conocemos' 'consideran' 'consola' 'consolas' 'construir'\n",
      " 'construyen' 'contempor' 'contenido' 'contenidos' 'contra'\n",
      " 'contraproducente' 'contrarias' 'contrase' 'contrataci' 'contratar'\n",
      " 'contratista' 'control' 'conversaciones' 'convertido' 'convierte' 'cord'\n",
      " 'coronavirus' 'correo' 'corrigen' 'cortana' 'costar' 'covid-19' 'crea'\n",
      " 'creador' 'creadores' 'crean' 'crear' 'crece' 'creed' 'creen' 'crunch'\n",
      " 'cticas' 'ctil' 'ctrica' 'cuando' 'cuenta' 'cuentas' 'cuidar' 'culo'\n",
      " 'culpa' 'culpan' 'culpas' 'cultura' 'curso' 'cybertruck' 'dame' 'dating'\n",
      " 'datos' 'debut' 'debuta' 'decesos' 'decidir' 'decir' 'declare' 'defensa'\n",
      " 'defiende' 'dejando' 'dejar' 'deje' 'demanda' 'dentro' 'denunciadas'\n",
      " 'denuncian' 'depende' 'derechista' 'desactiva' 'desaf' 'desaparici'\n",
      " 'desarrolla' 'desarrollan' 'desbloquea' 'descubre' 'descubren' 'desde'\n",
      " 'desplomaron' 'despu' 'destruye' 'desvincularse' 'detienen' 'deudas'\n",
      " 'diariamente' 'diccionario' 'dice' 'dicos' 'dientes' 'difunde' 'digital'\n",
      " 'digo' 'dijo' 'diputada' 'dirigirles' 'discoteca' 'disculpa' 'discurso'\n",
      " 'disney' 'dispositivos' 'dito' 'ditos' 'documentos' 'donan' 'donde'\n",
      " 'droga' 'duraderas' 'durante' 'editar' 'educativo' 'elecciones'\n",
      " 'eliminados' 'eliminar' 'elizabeth' 'elon' 'embajada' 'embarazosos'\n",
      " 'emojis' 'empleados' 'emprende' 'empresa' 'empresas' 'encontrar'\n",
      " 'encriptada' 'encuentran' 'enemigo' 'enganchan' 'enrollable' 'ensaya'\n",
      " 'ense' 'entra' 'entrada' 'entre' 'enviar' 'envuelta' 'errores' 'escala'\n",
      " 'escanean' 'escuchan' 'escuchar' 'escultor' 'espa' 'espionaje' 'estafeta'\n",
      " 'estar' 'este' 'estereotipos' 'estilo' 'estr' 'estudio' 'europa'\n",
      " 'excepto' 'exclusivo' 'exhiben' 'exoesqueleto' 'expectativas' 'expertos'\n",
      " 'explorer' 'explotaci' 'expuestos' 'expuso' 'extiende' 'faceapp'\n",
      " 'facebook' 'facial' 'fake' 'falla' 'fallas' 'falsa' 'falsas' 'falsos'\n",
      " 'favorito' 'fgem' 'fidelidad' 'fifa' 'filmados' 'filtraci' 'filtran'\n",
      " 'finge' 'firefox' 'fiscales' 'follow' 'fono' 'formato' 'fortnite'\n",
      " 'fortuna' 'foto' 'fotos' 'francia' 'fraudes' 'fuego' 'fuera' 'fueran'\n",
      " 'fundadores' 'futbol' 'futuro' 'game' 'gamers' 'ganancia' 'ganancias'\n",
      " 'garantizar' 'garc' 'genes' 'gente' 'gica' 'gigantes' 'global' 'golpe'\n",
      " 'google' 'graba' 'gracia' 'gratuito' 'grindr' 'gris' 'guatemala' 'guerra'\n",
      " 'gusano' 'hable' 'hace' 'hacen' 'hacer' 'hacia' 'hackean' 'hackeo' 'halc'\n",
      " 'harmonyos' 'harry' 'hasta' 'herramienta' 'historia' 'historias'\n",
      " 'homicidio' 'hong' 'huarache' 'huawei' 'hubo' 'huella' 'humanos' 'humor'\n",
      " 'igual' 'imaginada' 'impagables' 'imperio' 'impone' 'imponer' 'imposible'\n",
      " 'impuestos' 'impunidad' 'inclasificables' 'inclusivos' 'incluye'\n",
      " 'industria' 'infantil' 'infecta' 'influencers' 'informaci' 'ingresos'\n",
      " 'inmigrantes' 'inminente' 'inmune' 'instagram' 'instalaciones'\n",
      " 'inteligencia' 'intentan' 'interacciones' 'intercambia' 'interior'\n",
      " 'internacional' 'internet' 'intimidad' 'invasiva' 'inventan' 'investigan'\n",
      " 'invisible' 'ipad' 'iphone' 'ipod' 'irlanda' 'itunes' 'javier' 'jicas'\n",
      " 'jony' 'joven' 'juego' 'juegos' 'juguetito' 'justicia' 'juventus'\n",
      " 'konami' 'kong' 'laboral' 'laborales' 'laliga' 'lanza' 'lanzar' 'lares'\n",
      " 'lavarse' 'leer' 'legales' 'leyes' 'libras' 'licas' 'licenciatura'\n",
      " 'licos' 'like' 'limitaciones' 'limitar' 'linkedin' 'lite' 'litigar'\n",
      " 'llama' 'llamadas' 'llaman' 'lleg' 'llega' 'llegar' 'lucha' 'luchar'\n",
      " 'lugar' 'luna' 'malos' 'manchado' 'manzana' 'mara' 'maravillosa'\n",
      " 'masacres' 'masivo' 'mayor' 'medias' 'medio' 'medios' 'megamulta'\n",
      " 'megaupload' 'mejor' 'mejorar' 'mejores' 'meme' 'memes' 'memoria' 'menos'\n",
      " 'mensajes' 'messenger' 'metoo' 'mica' 'michoac' 'mico' 'micos'\n",
      " 'microsoft' 'mientras' 'miles' 'millennials' 'millonaria' 'millonario'\n",
      " 'millones' 'minecraft' 'miniatura' 'miniordenador' 'minuto' 'mira' 'misi'\n",
      " 'mobile' 'modo' 'monop' 'morir' 'motivo' 'mouse' 'muchos' 'multa'\n",
      " 'multada' 'mundo' 'musical' 'musk' 'nace' 'nacional' 'nasa' 'navegadores'\n",
      " 'navegar' 'ncipe' 'ndalo' 'ndalos' 'negociar' 'negocio' 'netflix' 'news'\n",
      " 'nico' 'niegan' 'nike' 'nimos' 'nintendo' 'nombre' 'nostalgia' 'noticias'\n",
      " 'notificaciones' 'notre' 'nube' 'nueva' 'nuevas' 'nuevo' 'nuevos'\n",
      " 'nutrici' 'objeto' 'odia' 'odio' 'oficina' 'opacas' 'original' 'oscar'\n",
      " 'otra' 'otros' 'pacientes' 'pagar' 'papeles' 'paperphone' 'para' 'parapl'\n",
      " 'parejas' 'pasajeros' 'pasan' 'pasaportes' 'peacock' 'pegue' 'peligran'\n",
      " 'perder' 'perdi' 'periodismo' 'periodistas' 'permitir' 'pero' 'persiana'\n",
      " 'personal' 'personalidad' 'personas' 'pesados' 'pese' 'pesos' 'phishing'\n",
      " 'photoshop' 'pichai' 'pikachu' 'pinterest' 'pionero' 'pista' 'pizzas'\n",
      " 'plan' 'plataforma' 'pleito' 'podr' 'pods' 'pone' 'port' 'portraits'\n",
      " 'post-tinder' 'prepara' 'presenta' 'presidente' 'presidentes' 'presunto'\n",
      " 'pretende' 'prev' 'primer' 'primera' 'privacidad' 'privadas' 'privado'\n",
      " 'problema' 'procede' 'productos' 'programa' 'proh' 'prohibido' 'prohibir'\n",
      " 'promover' 'promuevan' 'propio' 'propone' 'proponen' 'protagonizan'\n",
      " 'protestas' 'provenientes' 'prueba' 'publicar' 'publicidad' 'puede'\n",
      " 'puedo' 'queda' 'quiere' 'quieren' 'quitar' 'radio' 'ranking' 'rcito'\n",
      " 'reacciona' 'realidad' 'recargar' 'rechaza' 'recolecci' 'recolectar'\n",
      " 'reconoce' 'reconocimiento' 'recopilar' 'recuerdos' 'redes' 'reflexi'\n",
      " 'refuerzan' 'refugiados' 'registrar' 'reglas' 'relato' 'relevo' 'repite'\n",
      " 'reportan' 'repunta' 'resiste' 'respiro' 'responder' 'restauraci'\n",
      " 'resucitar' 'resume' 'reuni' 'reunirse' 'revela' 'rico' 'riesgo' 'rival'\n",
      " 'robarle' 'robocup' 'robots' 'romper' 'ropa' 'royale' 'rroga' 'rtalo'\n",
      " 'rtel' 'rueda' 'ruido' 'rumores' 'sabios' 'salud' 'saluda' 'saludables'\n",
      " 'salvajes' 'samsung' 'sanders' 'secci' 'secretar' 'seguir' 'segundo'\n",
      " 'seguridad' 'selfies' 'selfietype' 'senadores' 'sentir' 'seres'\n",
      " 'servicio' 'severas' 'sexismo' 'sexistas' 'sexo' 'sexuales' 'sexy'\n",
      " 'sheinbaum' 'sicos' 'siendo' 'siente' 'sindicato' 'sino' 'siri' 'sirva'\n",
      " 'skype' 'slim' 'smartphones' 'snapchat' 'snes' 'sobre' 'social'\n",
      " 'sociales' 'socio' 'software' 'sol-halloween' 'solos' 'sombras' 'sonido'\n",
      " 'sorpresa' 'sostiene' 'spotify' 'stadia' 'stark' 'stratfor' 'streaming'\n",
      " 'suben' 'sucesor' 'sufrir' 'suma' 'sundar' 'supera' 'supercomputadora'\n",
      " 'supremacismo' 'supuestas' 'surge' 'suscriptores' 'suspende' 'switch'\n",
      " 'tama' 'tambi' 'tarjeta' 'teclado' 'tecnol' 'tecnolog' 'tele' 'telegram'\n",
      " 'televisiones' 'temas' 'tendr' 'tenis' 'terminaremos' 'test' 'testigo'\n",
      " 'thrones' 'tica' 'ticas' 'tico' 'tienda' 'tienen' 'tiktok' 'tiktokers'\n",
      " 'tipo' 'tiro' 'todo' 'todos' 'tomar' 'tomarle' 'touch' 'trabajadoras'\n",
      " 'trabajar' 'tragedia' 'transcribi' 'transmitir' 'tras' 'traves' 'treinta'\n",
      " 'trump' 'tuits' 'tumblr' 'turing' 'twitch' 'twitter' 'uber'\n",
      " 'ultraderecha' 'unam' 'unesco' 'universidades' 'usan' 'usuarias'\n",
      " 'usuario' 'usuarios' 'usulas' 'vacuna' 'valdez' 'valer' 'velocidad'\n",
      " 'vencedor' 'vender' 'vendida' 'venes' 'venezuela' 'ventas' 'verificados'\n",
      " 'versi' 'versus' 'vibrar' 'vida' 'vidas' 'video' 'videoclips'\n",
      " 'videojuego' 'videojuegos' 'videos' 'viejo' 'viejos' 'vigilancia' 'vine'\n",
      " 'violaciones' 'violan' 'viral' 'viraliza' 'virtual' 'visas' 'visto'\n",
      " 'vivo' 'volver' 'walkman' 'warren' 'waze' 'wetransfer' 'whatsapp'\n",
      " 'whisky' 'wikileaks' 'windows' 'wordpress' 'world' 'xbox' 'xico' 'xodo'\n",
      " 'youtube' 'youtuber' 'youtubers' 'zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "palabrasCovid = PalabrasTopico(separaTopico('covid', dataTrain))\n",
    "AlfabetoCovid = numpy.unique(palabrasCovid)\n",
    "#print(len(palabrasCovid))\n",
    "#print(len(AlfabetoCovid))\n",
    "\n",
    "palabrasTecn = PalabrasTopico(separaTopico('tecnologia', dataTrain))\n",
    "AlfabetoTecn = numpy.unique(palabrasTecn)\n",
    "#print(len(palabrasTecn))\n",
    "#print(AlfabetoTecn)\n",
    "\n",
    "\n",
    "palabrasDep = PalabrasTopico(separaTopico('deportes', dataTrain))\n",
    "AlfabetoDep = numpy.unique(palabrasDep)\n",
    "\n",
    "Alfabeto = numpy.concatenate((AlfabetoCovid, AlfabetoTecn, AlfabetoDep))\n",
    "#Alfabeto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos las probabilidades a priori, haciendo cada una de estas equiprobabloe $ P(C) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCovid =  0.3333333333333333 PTecn =  0.3333333333333333 PDep =  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "PCovid = 1/3\n",
    "PTecn = 1/3\n",
    "PDep = 1/3\n",
    "print(\"PCovid = \", PCovid, \"PTecn = \", PTecn, \"PDep = \", PDep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos la frecuencia de palabras de cada atributo (palabra) $ P(A_{i} | C_{j}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00060976 0.00030488 0.00030488 ... 0.00182927 0.00030488 0.00030488]\n"
     ]
    }
   ],
   "source": [
    "ProbaCov = Probabilidad(palabrasCovid, AlfabetoCovid)\n",
    "print(ProbaCov)\n",
    "ProbaTecn = Probabilidad(palabrasTecn, AlfabetoTecn)\n",
    "ProbaDep = Probabilidad(palabrasDep, AlfabetoDep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"¿Te hacen faltan unos buenos audífonos? Checa las ofertas que encontramos en el\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indice =  <class 'numpy.int64'>\n",
      "Max =  4.966078332012773e-41\n",
      "Deportes\n"
     ]
    }
   ],
   "source": [
    "MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
