{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Los topicos se encuentran juntos en el dataframe, esta función los separa de acuerdo al tema, recibe el dataframe y devuelve una lista con los titulares de un solo tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separaTopico(topico, df): #Separa todos los encabezados de acuerdo a su tópico\n",
    "    listTopic = []\n",
    "    dataArr = numpy.asarray(df)\n",
    "    for i in range(len(dataArr)):\n",
    "        if(dataArr[i][1] == topico):\n",
    "            listTopic.append(dataArr[i][0])\n",
    "    return listTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cuando se tienen las palabras unicas de cada tópico, se eliminan caracteres especiales. Retorna una lista con las palabras que componen el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaTexto(text):\n",
    "    \"List all the word tokens in a text.\"\n",
    "    return re.findall('[a-zA-Z0-9\\-áéíóúÁÉÍÓÚ]{3,254}', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función busca eliminar palabras plurales para quedarnos únicamente con las singulares a través de la raíz de la palabra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RaizSingular(palabra):\n",
    "    ultima = len(palabra) -1\n",
    "    raiz = []\n",
    "    plural = False\n",
    "\n",
    "    if (palabra[ultima] == 's'): #Verificamos si la última letra es 's'\n",
    "        if (palabra[ultima - 1] == 'a' or \n",
    "            palabra[ultima - 1] == 'i' or\n",
    "            palabra[ultima - 1] == 'o' or\n",
    "            palabra[ultima - 1] == 'u'):\n",
    "            \n",
    "            for i in range(ultima):\n",
    "                    raiz.append(palabra[i])\n",
    "            \n",
    "            palabra = \"\".join(raiz)\n",
    "            return palabra\n",
    "            \n",
    "        if(palabra[ultima - 1] == 'e'):\n",
    "            if (palabra[ultima - 2] == 'c'):\n",
    "                for i in range(ultima - 2):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                raiz.append('z')\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "            \n",
    "            elif (palabra[ultima - 2] != 'a' or\n",
    "                palabra[ultima - 2] != 'e' or\n",
    "                palabra[ultima - 2] != 'i' or\n",
    "                palabra[ultima - 2] != 'o' or\n",
    "                palabra[ultima - 2] != 'u'):\n",
    "                \n",
    "                for i in range(ultima - 1):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "                                                                        \n",
    "    else:\n",
    "        return palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función sustituye las tildes de las vocales por unicamente las vocales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTildes(palabra):\n",
    "    s = palabra\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    \n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaPalabra(arrPalabras):\n",
    "    raiz = []\n",
    "    for i in range(len(arrPalabras)):\n",
    "        #raiz.append(RaizSingular(cleanTildes(arrPalabras[i])))\n",
    "        raiz.append(cleanTildes(arrPalabras[i]))\n",
    "    \n",
    "    return numpy.asarray(raiz, dtype=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cada encabezado corresponde a un documento, esta función separa cada palabra para agregarlas a un solo array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PalabrasTopico(listTopico): #Devuelve todas las palabras que aparecen en los titulares de las noticias \n",
    "    listHeadLines = []\n",
    "    for i in range(len(listTopico)):\n",
    "        #aux = nltk.word_tokenize(arrayTopico[i])\n",
    "        aux = limpiaTexto(listTopico[i])\n",
    "        #aux = limpiaPalabra()\n",
    "        for j in range(len(aux)):\n",
    "            aux2 = aux[j]\n",
    "            listHeadLines.append(aux2)\n",
    "            \n",
    "    return numpy.asarray(listHeadLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilidad(palabrasTopico, palabrasUnicasTopico): #Devuelve un array con la ocurrencias de cada única del tópico\n",
    "    Prob = numpy.zeros(len(palabrasUnicasTopico))\n",
    "    for i in range(len(palabrasUnicasTopico)):\n",
    "        Prob[i] = numpy.sum(palabrasTopico == palabrasUnicasTopico[i])\n",
    "        Prob[i] = Prob[i] /len(palabrasTopico)\n",
    "        \n",
    "    return Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recibe el vector de probabilidades y elige la máxima para finalmente devolver el tópico al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clasificador(arrayProbabilidades):\n",
    "    indice = numpy.where(arrayProbabilidades == max(arrayProbabilidades))[0][0]\n",
    "    #print(\"Max = \", max(arrayProbabilidades))\n",
    "    if (indice == 0):\n",
    "        return 'covid'\n",
    "    if (indice == 1):\n",
    "        return 'tecnologia'\n",
    "    if (indice == 2):\n",
    "        return 'deportes'\n",
    "    if (indice == 3):\n",
    "        return 'economia'\n",
    "    if (indice == 4):\n",
    "        return 'cultura'\n",
    "    return 'otros'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devuelve la probabilidad máxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep, palabrasEcon, AlfabetoEcon, palabrasCult, AlfabetoCult):\n",
    "    T = nltk.word_tokenize(tweet)\n",
    "    Proba = numpy.ones(5)\n",
    "\n",
    "    #Covid\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCovid == T[j]) + 1\n",
    "        Proba[0] = (aux) / (len(AlfabetoCovid) + aux)\n",
    "    \n",
    "    #Tecnologia\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasTecn == T[j]) \n",
    "        Proba[1] = (aux) / (len(AlfabetoTecn) + aux)\n",
    "\n",
    "    #Deportes\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasDep == T[j]) + 1\n",
    "        Proba[2] = (aux) / (len(AlfabetoDep) + aux)\n",
    "        \n",
    "    #Economía\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasEcon == T[j]) + 1\n",
    "        Proba[3] = (aux) / (len(AlfabetoEcon) + aux)\n",
    "        \n",
    "    #Cultura\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCult == T[j]) + 1\n",
    "        Proba[4] = (aux) / (len(AlfabetoCult) + aux)\n",
    "\n",
    "    for p in range(len(Proba)):\n",
    "        Proba[p] = Proba[p] * 1/5\n",
    "        \n",
    "    return Clasificador(Proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaTweet(df):\n",
    "    listTweets = df.Tweets.values.tolist()\n",
    "    pattern = re.compile('https?://(?:[-\\w./]|(?:%\\da-fA-F{2}))+')\n",
    "    cleanTweets =[]\n",
    "    \n",
    "    for tweet in listTweets:\n",
    "        res = re.findall(pattern, tweet)\n",
    "        if (len(res) != 0):\n",
    "            aux = tweet.replace(res[0], '')\n",
    "            cleanTweets.append(aux)\n",
    "        else:\n",
    "            cleanTweets.append(tweet)\n",
    "            \n",
    "    for tweet in cleanTweets:\n",
    "        if(len(tweet) == 0):\n",
    "            cleanTweets.remove(tweet)\n",
    "    \n",
    "    return numpy.asarray(cleanTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcesamiento(listaTitulares):\n",
    "    titulares = []\n",
    "    for titular in listaTitulares:\n",
    "        aux = limpiaTexto(titular)\n",
    "        titulares.append(str.join(' ', aux))\n",
    "    \n",
    "    return titulares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abrimos el dataset con pandas\n",
    "Se abre el dataset de titulares y se realiza un preprocesamiento el cual consiste en eliminar tildes y eliminar palabras que tengan una longitud menor a tres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Tópico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cierran rastro de Mérida tras detectar 17 cont...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El “peor escenario” para Tabasco eran 200 muer...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jalisco instalará espacios para enfermos no gr...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Guerrero no está listo para iniciar la “nueva ...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reportan la muerte de 364 personas por covid-1...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>“El Cascanueces” de Disney: Cuento de hadas, m...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>Rock, blues y música alternativa en Sinaloa</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7850</th>\n",
       "      <td>María Mercedes Coroy es “Malinche” en Canal Once</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7852</th>\n",
       "      <td>“Motivos para el canto y la danza. Poesía del 68”</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7854</th>\n",
       "      <td>Los pequeños mundos de Kandinsky en México</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3926 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Titular   Tópico\n",
       "0     Cierran rastro de Mérida tras detectar 17 cont...    covid\n",
       "2     El “peor escenario” para Tabasco eran 200 muer...    covid\n",
       "4     Jalisco instalará espacios para enfermos no gr...    covid\n",
       "6     Guerrero no está listo para iniciar la “nueva ...    covid\n",
       "8     Reportan la muerte de 364 personas por covid-1...    covid\n",
       "...                                                 ...      ...\n",
       "7846  “El Cascanueces” de Disney: Cuento de hadas, m...  cultura\n",
       "7848        Rock, blues y música alternativa en Sinaloa  cultura\n",
       "7850   María Mercedes Coroy es “Malinche” en Canal Once  cultura\n",
       "7852  “Motivos para el canto y la danza. Poesía del 68”  cultura\n",
       "7854        Los pequeños mundos de Kandinsky en México   cultura\n",
       "\n",
       "[3926 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = ['Titular', 'Tópico']\n",
    "dataTrain = pandas.read_csv('.//TitularesTrain//titulares.csv', names=nombres)\n",
    "dataTrain = dataTrain.dropna()\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataTrain.Tópico.values.tolist()\n",
    "X = dataTrain.Titular.values.tolist()\n",
    "\n",
    "x, X_test, y, y_test = train_test_split(X, Y, test_size=0.3, random_state=100)\n",
    "\n",
    "unir = []\n",
    "for i in range(len(x)):\n",
    "    unir.append([x[i], y[i]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTopic(topico, lista):\n",
    "    tema = []\n",
    "    for i in range(len(lista)):\n",
    "        if (lista[i][1] == topico):\n",
    "            tema.append(lista[i][0])\n",
    "    \n",
    "    return tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separamos del dataset por tópicos y a cada encabezado lo separamos por palabra de los documentos eliminando caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfabeto Covid =  2261\n",
      "Alfabeto Tecnología =  534\n",
      "Alfabeto Deportes =  2446\n",
      "Alfabeto Economía =  1988\n",
      "Alfabeto Cultura =  1248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8477,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabrasCovid = limpiaPalabra(PalabrasTopico(splitTopic('covid', unir)))\n",
    "AlfabetoCovid = numpy.unique(palabrasCovid)\n",
    "print(\"Alfabeto Covid = \", len(AlfabetoCovid))\n",
    "\n",
    "palabrasTecn = limpiaPalabra(PalabrasTopico(splitTopic('tecnologia', unir)))\n",
    "AlfabetoTecn = numpy.unique(palabrasTecn)\n",
    "print(\"Alfabeto Tecnología = \", len(AlfabetoTecn))\n",
    "\n",
    "palabrasDep = limpiaPalabra(PalabrasTopico(splitTopic('deportes', unir)))\n",
    "AlfabetoDep = numpy.unique(palabrasDep)\n",
    "print(\"Alfabeto Deportes = \", len(AlfabetoDep))\n",
    "\n",
    "palabrasEcon = limpiaPalabra(PalabrasTopico(splitTopic('economia', unir)))\n",
    "AlfabetoEcon = numpy.unique(palabrasEcon)\n",
    "print(\"Alfabeto Economía = \", len(AlfabetoEcon))\n",
    "\n",
    "palabrasCult = limpiaPalabra(PalabrasTopico(splitTopic('cultura', unir)))\n",
    "AlfabetoCult = numpy.unique(palabrasCult)\n",
    "print(\"Alfabeto Cultura = \", len(AlfabetoCult))\n",
    "\n",
    "Alfabeto = numpy.concatenate((AlfabetoCovid, AlfabetoTecn, AlfabetoDep, AlfabetoEcon, AlfabetoCult))\n",
    "Alfabeto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos las probabilidades a priori, haciendo cada una de estas equiprobabloe $ P(C) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCovid =  0.2 PTecn =  0.2 PDep =  0.2 PEcon =  0.2 PCult =  0.2\n"
     ]
    }
   ],
   "source": [
    "PCovid = 1/5\n",
    "PTecn = 1/5\n",
    "PDep = 1/5\n",
    "PEcon = 1/5\n",
    "PCult = 1/5\n",
    "print(\"PCovid = \", PCovid, \"PTecn = \", PTecn, \"PDep = \", PDep, \"PEcon = \", PEcon, \"PCult = \", PCult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178\n",
      "1178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[134, 186,   4,  19,   3],\n",
       "       [  5, 138,   2,   3,   0],\n",
       "       [ 17, 248,  35,  21,   1],\n",
       "       [  5, 212,   5,  90,   4],\n",
       "       [  5,  33,   1,   5,   2]], dtype=int64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(len(X_test)):\n",
    "    result.append(MAP(X_test[i], palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep, palabrasEcon, AlfabetoEcon, palabrasCult, AlfabetoCult))\n",
    "    \n",
    "resultClass = numpy.asarray(result)\n",
    "\n",
    "print(len(resultClass))\n",
    "print(len(y_test))\n",
    "\n",
    "confusion_matrix(y_test, resultClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 678 entries, 0 to 677\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Tweets  678 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 10.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/6cmSHgMXJo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @AkemiLook: UPDATE: I just got off a call w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @AkemiLook: CALLING LA LAWYERS: LA is in de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pablo Guerrero Cañez, luchador social de Mexic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Baby Face https://t.co/Vp2AILGyTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>⟚ Ղ⚶▸ ̋ ᢇា឵឴┬┴┬┴┤(･_├┬┴┬┴ ͭ  ͅ ⟘≷\\n\\ny̆̆̆̆̆̆̆̆...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>https://t.co/ss4PjrxFGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>https://t.co/eb6dt2R3QD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>\"Pita Amor\" es tendencia porque hoy se recuerd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets\n",
       "0                                               Tweets\n",
       "1                              https://t.co/6cmSHgMXJo\n",
       "2    RT @AkemiLook: UPDATE: I just got off a call w...\n",
       "3    RT @AkemiLook: CALLING LA LAWYERS: LA is in de...\n",
       "4    Pablo Guerrero Cañez, luchador social de Mexic...\n",
       "..                                                 ...\n",
       "673                  Baby Face https://t.co/Vp2AILGyTA\n",
       "674  ⟚ Ղ⚶▸ ̋ ᢇា឵឴┬┴┬┴┤(･_├┬┴┬┴ ͭ  ͅ ⟘≷\\n\\ny̆̆̆̆̆̆̆̆...\n",
       "675                            https://t.co/ss4PjrxFGB\n",
       "676                            https://t.co/eb6dt2R3QD\n",
       "677  \"Pita Amor\" es tendencia porque hoy se recuerd...\n",
       "\n",
       "[678 rows x 1 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTweets = pandas.read_csv(\".//TweetsTest//out.txt\", names=['Tweets'])\n",
    "dataTweets = dataTweets.dropna()\n",
    "dataTweets.info()\n",
    "dataTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n"
     ]
    }
   ],
   "source": [
    "tweets = limpiaTweet(dataTweets)\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Covid = []\n",
    "Tecnologia = []\n",
    "Deportes = []\n",
    "Economia =[]\n",
    "Cultura = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    result = MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep, palabrasEcon, AlfabetoEcon, palabrasCult, AlfabetoCult)\n",
    "    if result == 'covid':\n",
    "        Covid.append([tweet, result])\n",
    "    if result == 'tecnologia':\n",
    "        Tecnologia.append([tweet, result])\n",
    "    if result == 'deportes':\n",
    "        Deportes.append([tweet, result])\n",
    "    if result == 'economia':\n",
    "        Economia.append([tweet, result])\n",
    "    if result == 'cultura':\n",
    "        Cultura.append([tweet, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid =  29\n",
      "Tecnología =  3\n",
      "Deportes =  2\n",
      "Economía =  0\n",
      "Cultura =  588\n"
     ]
    }
   ],
   "source": [
    "print(\"Covid = \", len(Covid))\n",
    "print(\"Tecnología = \", len(Tecnologia))\n",
    "print(\"Deportes = \", len(Deportes))\n",
    "print(\"Economía = \", len(Economia))\n",
    "print(\"Cultura = \", len(Cultura))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
