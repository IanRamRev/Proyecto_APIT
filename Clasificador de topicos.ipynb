{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Los topicos se encuentran juntos en el dataframe, esta función los separa de acuerdo al tema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separaTopico(topico, df): #Separa todos los encabezados de acuerdo a su tópico\n",
    "    listTopic = []\n",
    "    dataArr = numpy.asarray(df)\n",
    "    for i in range(len(dataArr)):\n",
    "        if(dataArr[i][1] == topico):\n",
    "            listTopic.append(dataArr[i][0])\n",
    "    return listTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cuando se tienen las palabras unicas de cada tópico, se eliminan caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaTexto(text):\n",
    "    \"List all the word tokens in a text.\"\n",
    "    return re.findall('[a-zA-Z0-9\\-áéíóúÁÉÍÓÚ]{4,254}', text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función busca eliminar palabras plurales para quedarnos únicamente con las singulares a través de la raíz de la palabra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RaizSingular(palabra):\n",
    "    ultima = len(palabra) -1\n",
    "    raiz = []\n",
    "    plural = False\n",
    "\n",
    "    if (palabra[ultima] == 's'): #Verificamos si la última letra es 's'\n",
    "        if (palabra[ultima - 1] == 'a' or \n",
    "            palabra[ultima - 1] == 'i' or\n",
    "            palabra[ultima - 1] == 'o' or\n",
    "            palabra[ultima - 1] == 'u'):\n",
    "            \n",
    "            for i in range(ultima):\n",
    "                    raiz.append(palabra[i])\n",
    "            \n",
    "            palabra = \"\".join(raiz)\n",
    "            return palabra\n",
    "            \n",
    "        if(palabra[ultima - 1] == 'e'):\n",
    "            if (palabra[ultima - 2] == 'c'):\n",
    "                for i in range(ultima - 2):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                raiz.append('z')\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "            \n",
    "            elif (palabra[ultima - 2] != 'a' or\n",
    "                palabra[ultima - 2] != 'e' or\n",
    "                palabra[ultima - 2] != 'i' or\n",
    "                palabra[ultima - 2] != 'o' or\n",
    "                palabra[ultima - 2] != 'u'):\n",
    "                \n",
    "                for i in range(ultima - 1):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "                                                                        \n",
    "    else:\n",
    "        return palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función sustituye las tildes de las vocales por unicamente las vocales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTildes(palabra):\n",
    "    s = palabra\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    \n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaPalabra(arrPalabras):\n",
    "    raiz = []\n",
    "    for i in range(len(arrPalabras)):\n",
    "        raiz.append(RaizSingular(cleanTildes(arrPalabras[i])))\n",
    "    \n",
    "    return numpy.asarray(raiz, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cada encabezado corresponde a un documento, esta función separa cada palabra para agregarlas a un solo array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PalabrasTopico(listTopico): #Devuelve todas las palabras que aparecen en los titulares de las noticias \n",
    "    listHeadLines = []\n",
    "    for i in range(len(listTopico)):\n",
    "        #aux = nltk.word_tokenize(arrayTopico[i])\n",
    "        aux = limpiaTexto(listTopico[i])\n",
    "        for j in range(len(aux)):\n",
    "            aux2 = aux[j]\n",
    "            listHeadLines.append(aux2)\n",
    "            \n",
    "    return numpy.asarray(listHeadLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilidad(palabrasTopico, palabrasUnicasTopico): #Devuelve un array con la ocurrencias de cada única del tópico\n",
    "    Prob = numpy.zeros(len(palabrasUnicasTopico))\n",
    "    for i in range(len(palabrasUnicasTopico)):\n",
    "        Prob[i] = numpy.sum(palabrasTopico == palabrasUnicasTopico[i])\n",
    "        Prob[i] = Prob[i] /len(palabrasTopico)\n",
    "        \n",
    "    return Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recibe el vector de probabilidades y elige la máxima para finalmente devolver el tópico al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clasificador(arrayProbabilidades):\n",
    "    indice = numpy.where(arrayProbabilidades == max(arrayProbabilidades))[0][0]\n",
    "    print(\"Indice = \", type(indice))\n",
    "    print(\"Max = \", max(arrayProbabilidades))\n",
    "    if (indice == 0):\n",
    "        print(\"Covid\")\n",
    "    if (indice == 1):\n",
    "        print(\"Tecnología\")\n",
    "    if (indice == 2):\n",
    "        print(\"Deportes\")\n",
    "    if (indice == 3):\n",
    "        print(\"Economía\")\n",
    "    if (indice == 4):\n",
    "        print(\"Cultura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep):\n",
    "    T = nltk.word_tokenize(tweet)\n",
    "    Proba = numpy.ones(5)\n",
    "\n",
    "    #Covid\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCovid == T[j]) + 1\n",
    "        Proba[0] *= (aux + 1) / (len(AlfabetoCovid) + aux)\n",
    "    \n",
    "    #Tecnologia\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasTecn == T[j]) \n",
    "        Proba[1] *= (aux + 1) / (len(AlfabetoTecn) + aux)\n",
    "\n",
    "    #Deportes\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasDep == T[j]) + 1\n",
    "        Proba[2] *= (aux + 1) / (len(AlfabetoDep) + aux)\n",
    "        \n",
    "    #Economía\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasEcon == T[j]) + 1\n",
    "        Proba[3] *= (aux + 1) / (len(AlfabetoEcon) + aux)\n",
    "        \n",
    "    #Economía\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCult == T[j]) + 1\n",
    "        Proba[4] *= (aux + 1) / (len(AlfabetoCult) + aux)\n",
    "\n",
    "    for p in range(len(Proba)):\n",
    "        Proba[p] = Proba[p] * 1/5\n",
    "        \n",
    "    return Clasificador(Proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abrimos el dataset con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Tópico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pemex suma 251 decesos, mil 538 casos confirma...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semáforos para nueva normalidad tendrán “pulso...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>España inicia 10 días de luto por las víctimas...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Pero qué necesidad”, dice Calderón sobre la f...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OIT: el desempleo será mayor en países como Mé...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>“Balam Antsetik. La Segunda Era”, de Margarita...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>“The Angel”: Una promesa incumplida</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137</th>\n",
       "      <td>Iris Bringas con “Rockdrigo” en la Biblioteca ...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>Gabinete Rococó, en el Museo Nacional de San C...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>\\r\\nLa estrategia de AMLO “empuja al país al a...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Titular   Tópico\n",
       "0     Pemex suma 251 decesos, mil 538 casos confirma...    covid\n",
       "1     Semáforos para nueva normalidad tendrán “pulso...    covid\n",
       "2     España inicia 10 días de luto por las víctimas...    covid\n",
       "3     “Pero qué necesidad”, dice Calderón sobre la f...    covid\n",
       "4     OIT: el desempleo será mayor en países como Mé...    covid\n",
       "...                                                 ...      ...\n",
       "4135  “Balam Antsetik. La Segunda Era”, de Margarita...  cultura\n",
       "4136                “The Angel”: Una promesa incumplida  cultura\n",
       "4137  Iris Bringas con “Rockdrigo” en la Biblioteca ...  cultura\n",
       "4138  Gabinete Rococó, en el Museo Nacional de San C...  cultura\n",
       "4139  \\r\\nLa estrategia de AMLO “empuja al país al a...  cultura\n",
       "\n",
       "[4140 rows x 2 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = ['Titular', 'Tópico']\n",
    "dataTrain = pandas.read_csv('titulares.csv', names=nombres)\n",
    "dataTrain = dataTrain.dropna()\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pemex', 'suma', 'decesos', ..., 'mundo', 'murió', 'covid-19'],\n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encabezadosCov = separaTopico('covid', dataTrain)\n",
    "alfaCov = PalabrasTopico(encabezadosCov)\n",
    "alfaCov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separamos del dataset por tópicos y a cada encabezado lo separamos por palabra de los documentos eliminando caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilícito\n"
     ]
    }
   ],
   "source": [
    "palabrasCovid = PalabrasTopico(separaTopico('covid', dataTrain))\n",
    "AlfabetoCovid = numpy.unique(palabrasCovid)\n",
    "\n",
    "palabrasTecn = PalabrasTopico(separaTopico('tecnologia', dataTrain))\n",
    "AlfabetoTecn = numpy.unique(palabrasTecn)\n",
    "#print(len(palabrasTecn))\n",
    "#print(AlfabetoTecn)\n",
    "\n",
    "palabrasDep = PalabrasTopico(separaTopico('deportes', dataTrain))\n",
    "AlfabetoDep = numpy.unique(palabrasDep)\n",
    "\n",
    "palabrasEcon = PalabrasTopico(separaTopico('economia', dataTrain))\n",
    "AlfabetoEcon1 = numpy.unique(palabrasEcon)\n",
    "#print(\"Antes:\", len(palabrasEcon))\n",
    "#print(AlfabetoEcon1)\n",
    "\n",
    "#auxPalabras = limpiaPalabra(palabrasEcon)\n",
    "#AlfabetoEcon2 = numpy.unique(auxPalabras)\n",
    "#print(\"Despues: \", len(auxPalabras))\n",
    "#print(AlfabetoEcon2)\n",
    "\n",
    "palabrasCult = PalabrasTopico(separaTopico('cultura', dataTrain))\n",
    "AlfabetoCult = numpy.unique(palabrasCult)\n",
    "print(AlfabetoCult[789])\n",
    "\n",
    "Alfabeto = numpy.concatenate((AlfabetoCovid, AlfabetoTecn, AlfabetoDep, AlfabetoEcon, AlfabetoCult))\n",
    "#Alfabeto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos las probabilidades a priori, haciendo cada una de estas equiprobabloe $ P(C) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCovid =  0.2 PTecn =  0.2 PDep =  0.2 PEcon =  0.2 PCult =  0.2\n"
     ]
    }
   ],
   "source": [
    "PCovid = 1/5\n",
    "PTecn = 1/5\n",
    "PDep = 1/5\n",
    "PEcon = 1/5\n",
    "PCult = 1/5\n",
    "print(\"PCovid = \", PCovid, \"PTecn = \", PTecn, \"PDep = \", PDep, \"PEcon = \", PEcon, \"PCult = \", PCult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos la frecuencia de palabras de cada atributo (palabra) $ P(A_{i} | C_{j}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00025478 0.00012739 0.00012739 ... 0.00012739 0.00025478 0.00012739]\n"
     ]
    }
   ],
   "source": [
    "ProbaCov = Probabilidad(palabrasCovid, AlfabetoCovid)\n",
    "print(ProbaCov)\n",
    "ProbaTecn = Probabilidad(palabrasTecn, AlfabetoTecn)\n",
    "ProbaDep = Probabilidad(palabrasDep, AlfabetoDep)\n",
    "ProbaEcon = Probabilidad(palabrasEcon, AlfabetoEcon)\n",
    "ProbaCult = Probabilidad(palabrasCult, AlfabetoCult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"festival\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indice =  <class 'numpy.int64'>\n",
      "Max =  0.0018411967779056386\n",
      "Cultura\n"
     ]
    }
   ],
   "source": [
    "MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
