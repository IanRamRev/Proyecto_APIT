{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Los topicos se encuentran juntos en el dataframe, esta función los separa de acuerdo al tema, recibe el dataframe y devuelve una lista con los titulares de un solo tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separaTopico(topico, df): #Separa todos los encabezados de acuerdo a su tópico\n",
    "    listTopic = []\n",
    "    dataArr = numpy.asarray(df)\n",
    "    for i in range(len(dataArr)):\n",
    "        if(dataArr[i][1] == topico):\n",
    "            listTopic.append(dataArr[i][0])\n",
    "    return listTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cuando se tienen las palabras unicas de cada tópico, se eliminan caracteres especiales. Retorna una lista con las palabras que componen el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaTexto(text):\n",
    "    \"List all the word tokens in a text.\"\n",
    "    return re.findall('[a-zA-Z0-9\\-áéíóúÁÉÍÓÚ]{3,254}', text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función busca eliminar palabras plurales para quedarnos únicamente con las singulares a través de la raíz de la palabra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RaizSingular(palabra):\n",
    "    ultima = len(palabra) -1\n",
    "    raiz = []\n",
    "    plural = False\n",
    "\n",
    "    if (palabra[ultima] == 's'): #Verificamos si la última letra es 's'\n",
    "        if (palabra[ultima - 1] == 'a' or \n",
    "            palabra[ultima - 1] == 'i' or\n",
    "            palabra[ultima - 1] == 'o' or\n",
    "            palabra[ultima - 1] == 'u'):\n",
    "            \n",
    "            for i in range(ultima):\n",
    "                    raiz.append(palabra[i])\n",
    "            \n",
    "            palabra = \"\".join(raiz)\n",
    "            return palabra\n",
    "            \n",
    "        if(palabra[ultima - 1] == 'e'):\n",
    "            if (palabra[ultima - 2] == 'c'):\n",
    "                for i in range(ultima - 2):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                raiz.append('z')\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "            \n",
    "            elif (palabra[ultima - 2] != 'a' or\n",
    "                palabra[ultima - 2] != 'e' or\n",
    "                palabra[ultima - 2] != 'i' or\n",
    "                palabra[ultima - 2] != 'o' or\n",
    "                palabra[ultima - 2] != 'u'):\n",
    "                \n",
    "                for i in range(ultima - 1):\n",
    "                    raiz.append(palabra[i])\n",
    "                    \n",
    "                palabra = \"\".join(raiz)\n",
    "                return palabra\n",
    "                                                                        \n",
    "    else:\n",
    "        return palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Esta función sustituye las tildes de las vocales por unicamente las vocales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTildes(palabra):\n",
    "    s = palabra\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    \n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaPalabra(arrPalabras):\n",
    "    raiz = []\n",
    "    for i in range(len(arrPalabras)):\n",
    "        #raiz.append(RaizSingular(cleanTildes(arrPalabras[i])))\n",
    "        raiz.append(cleanTildes(arrPalabras[i]))\n",
    "    \n",
    "    return numpy.asarray(raiz, dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'u' 'x' 'i' 'l' 'i' 'o']\n"
     ]
    }
   ],
   "source": [
    "aux = \"auxílio\"\n",
    "print(limpiaPalabra(aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cada encabezado corresponde a un documento, esta función separa cada palabra para agregarlas a un solo array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PalabrasTopico(listTopico): #Devuelve todas las palabras que aparecen en los titulares de las noticias \n",
    "    listHeadLines = []\n",
    "    for i in range(len(listTopico)):\n",
    "        #aux = nltk.word_tokenize(arrayTopico[i])\n",
    "        aux = limpiaTexto(listTopico[i])\n",
    "        #aux = limpiaPalabra()\n",
    "        for j in range(len(aux)):\n",
    "            aux2 = aux[j]\n",
    "            listHeadLines.append(aux2)\n",
    "            \n",
    "    return numpy.asarray(listHeadLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilidad(palabrasTopico, palabrasUnicasTopico): #Devuelve un array con la ocurrencias de cada única del tópico\n",
    "    Prob = numpy.zeros(len(palabrasUnicasTopico))\n",
    "    for i in range(len(palabrasUnicasTopico)):\n",
    "        Prob[i] = numpy.sum(palabrasTopico == palabrasUnicasTopico[i])\n",
    "        Prob[i] = Prob[i] /len(palabrasTopico)\n",
    "        \n",
    "    return Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recibe el vector de probabilidades y elige la máxima para finalmente devolver el tópico al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clasificador(arrayProbabilidades):\n",
    "    indice = numpy.where(arrayProbabilidades == max(arrayProbabilidades))[0][0]\n",
    "    #print(\"Max = \", max(arrayProbabilidades))\n",
    "    if (indice == 0):\n",
    "        #print(\"Covid\")\n",
    "        return 'covid'\n",
    "    if (indice == 1):\n",
    "        #print(\"Tecnología\")\n",
    "        return 'tecnologia'\n",
    "    if (indice == 2):\n",
    "        #print(\"Deportes\")\n",
    "        return 'deportes'\n",
    "    if (indice == 3):\n",
    "        #print(\"Economía\")\n",
    "        return 'economia'\n",
    "    if (indice == 4):\n",
    "        #print(\"Cultura\")\n",
    "        return 'cultura'\n",
    "    return 'otros'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devuelve la probabilidad máxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(tweet, palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep):\n",
    "    T = nltk.word_tokenize(tweet)\n",
    "    Proba = numpy.ones(5)\n",
    "\n",
    "    #Covid\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCovid == T[j]) + 1\n",
    "        Proba[0] *= (aux + 1) / (len(AlfabetoCovid) + aux)\n",
    "    \n",
    "    #Tecnologia\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasTecn == T[j]) \n",
    "        Proba[1] *= (aux + 1) / (len(AlfabetoTecn) + aux)\n",
    "\n",
    "    #Deportes\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasDep == T[j]) + 1\n",
    "        Proba[2] *= (aux + 1) / (len(AlfabetoDep) + aux)\n",
    "        \n",
    "    #Economía\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasEcon == T[j]) + 1\n",
    "        Proba[3] *= (aux + 1) / (len(AlfabetoEcon) + aux)\n",
    "        \n",
    "    #Economía\n",
    "    for j in range(len(T)):\n",
    "        aux = numpy.sum(palabrasCult == T[j]) + 1\n",
    "        Proba[4] *= (aux + 1) / (len(AlfabetoCult) + aux)\n",
    "\n",
    "    for p in range(len(Proba)):\n",
    "        Proba[p] = Proba[p] * 1/5\n",
    "        \n",
    "    return Clasificador(Proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abrimos el dataset con pandas\n",
    "Se abre el dataset de titulares y se realiza un preprocesamiento el cual consiste en eliminar tildes y eliminar palabras que tengan una longitud menor a tres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Tópico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cierran rastro de Mérida tras detectar 17 cont...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El “peor escenario” para Tabasco eran 200 muer...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jalisco instalará espacios para enfermos no gr...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Guerrero no está listo para iniciar la “nueva ...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reportan la muerte de 364 personas por covid-1...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>“El Cascanueces” de Disney: Cuento de hadas, m...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>Rock, blues y música alternativa en Sinaloa</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7850</th>\n",
       "      <td>María Mercedes Coroy es “Malinche” en Canal Once</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7852</th>\n",
       "      <td>“Motivos para el canto y la danza. Poesía del 68”</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7854</th>\n",
       "      <td>Los pequeños mundos de Kandinsky en México</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3926 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Titular   Tópico\n",
       "0     Cierran rastro de Mérida tras detectar 17 cont...    covid\n",
       "2     El “peor escenario” para Tabasco eran 200 muer...    covid\n",
       "4     Jalisco instalará espacios para enfermos no gr...    covid\n",
       "6     Guerrero no está listo para iniciar la “nueva ...    covid\n",
       "8     Reportan la muerte de 364 personas por covid-1...    covid\n",
       "...                                                 ...      ...\n",
       "7846  “El Cascanueces” de Disney: Cuento de hadas, m...  cultura\n",
       "7848        Rock, blues y música alternativa en Sinaloa  cultura\n",
       "7850   María Mercedes Coroy es “Malinche” en Canal Once  cultura\n",
       "7852  “Motivos para el canto y la danza. Poesía del 68”  cultura\n",
       "7854        Los pequeños mundos de Kandinsky en México   cultura\n",
       "\n",
       "[3926 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = ['Titular', 'Tópico']\n",
    "dataTrain = pandas.read_csv('.//TitularesTrain//titulares.csv', names=nombres)\n",
    "dataTrain = dataTrain.dropna()\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separamos del dataset por tópicos y a cada encabezado lo separamos por palabra de los documentos eliminando caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199\n"
     ]
    }
   ],
   "source": [
    "palabrasCovid = limpiaPalabra(PalabrasTopico(separaTopico('covid', dataTrain)))\n",
    "AlfabetoCovid = numpy.unique(palabrasCovid)\n",
    "print(len(palabrasCovid))\n",
    "\n",
    "\n",
    "palabrasTecn = limpiaPalabra(PalabrasTopico(separaTopico('tecnologia', dataTrain)))\n",
    "AlfabetoTecn = numpy.unique(palabrasTecn)\n",
    "#print(len(palabrasTecn))\n",
    "#print(AlfabetoTecn)\n",
    "\n",
    "palabrasDep = limpiaPalabra(PalabrasTopico(separaTopico('deportes', dataTrain)))\n",
    "AlfabetoDep = numpy.unique(palabrasDep)\n",
    "\n",
    "palabrasEcon = limpiaPalabra(PalabrasTopico(separaTopico('economia', dataTrain)))\n",
    "AlfabetoEcon = numpy.unique(palabrasEcon)\n",
    "\n",
    "palabrasCult = limpiaPalabra(PalabrasTopico(separaTopico('cultura', dataTrain)))\n",
    "AlfabetoCult = numpy.unique(palabrasCult)\n",
    "\n",
    "Alfabeto = numpy.concatenate((AlfabetoCovid, AlfabetoTecn, AlfabetoDep, AlfabetoEcon, AlfabetoCult))\n",
    "#Alfabeto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos las probabilidades a priori, haciendo cada una de estas equiprobabloe $ P(C) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCovid =  0.2 PTecn =  0.2 PDep =  0.2 PEcon =  0.2 PCult =  0.2\n"
     ]
    }
   ],
   "source": [
    "PCovid = 1/5\n",
    "PTecn = 1/5\n",
    "PDep = 1/5\n",
    "PEcon = 1/5\n",
    "PCult = 1/5\n",
    "print(\"PCovid = \", PCovid, \"PTecn = \", PTecn, \"PDep = \", PDep, \"PEcon = \", PEcon, \"PCult = \", PCult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtenemos la frecuencia de palabras de cada atributo (palabra) $ P(A_{i} | C_{j}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00021741 0.00054354 0.00010871 ... 0.00054354 0.00010871 0.00010871]\n"
     ]
    }
   ],
   "source": [
    "ProbaCov = Probabilidad(palabrasCovid, AlfabetoCovid)\n",
    "print(ProbaCov)\n",
    "ProbaTecn = Probabilidad(palabrasTecn, AlfabetoTecn)\n",
    "ProbaDep = Probabilidad(palabrasDep, AlfabetoDep)\n",
    "ProbaEcon = Probabilidad(palabrasEcon, AlfabetoEcon)\n",
    "ProbaCult = Probabilidad(palabrasCult, AlfabetoCult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Tópico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inicia esta semana la entrega de créditos a mi...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El IPN retomará clases el 1 de junio y se exte...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coparmex urge a trabajadores a que exijan el s...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMLO prevé viajar a EU para agradecer a Trump ...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>La pandemia saca a flote la desigualdad en acc...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Una breve historia fílmica de Venom</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>En la cocina de Pati Jinich, chef mexicana rec...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Actividades por el homenaje a Arreola y estren...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>“Balam Antsetik. La Segunda Era”, de Margarita...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>\\r\\nLlamado a no creer en el covid-19 causa re...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titular   Tópico\n",
       "0    Inicia esta semana la entrega de créditos a mi...    covid\n",
       "2    El IPN retomará clases el 1 de junio y se exte...    covid\n",
       "4    Coparmex urge a trabajadores a que exijan el s...    covid\n",
       "6    AMLO prevé viajar a EU para agradecer a Trump ...    covid\n",
       "8    La pandemia saca a flote la desigualdad en acc...    covid\n",
       "..                                                 ...      ...\n",
       "418                Una breve historia fílmica de Venom  cultura\n",
       "420  En la cocina de Pati Jinich, chef mexicana rec...  cultura\n",
       "422  Actividades por el homenaje a Arreola y estren...  cultura\n",
       "424  “Balam Antsetik. La Segunda Era”, de Margarita...  cultura\n",
       "426  \\r\\nLlamado a no creer en el covid-19 causa re...  cultura\n",
       "\n",
       "[214 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTest = pandas.read_csv('.//TitularesTrain//Test.csv', names=nombres)\n",
    "dataTest = dataTest.dropna()\n",
    "dataTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcesamiento(listaTitulares):\n",
    "    titulares = []\n",
    "    for titular in listaTitulares:\n",
    "        aux = limpiaTexto(titular)\n",
    "        titulares.append(str.join(' ', aux))\n",
    "    \n",
    "    return titulares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid =  35 Tecnología =  25 Deportes =  40 Economia =  65 Cultura=  49\n"
     ]
    }
   ],
   "source": [
    "listTest = dataTest.Titular.values.tolist()\n",
    "arrTest = numpy.asarray(preProcesamiento(listTest))\n",
    "\n",
    "covTrue = 0\n",
    "TecnTrue = 0\n",
    "DepTrue = 0\n",
    "EconTrue = 0\n",
    "CultTrue = 0\n",
    "for i in range(len(arrTest)):\n",
    "    if (arrTestTopic[i] == 'covid'):\n",
    "        covTrue += 1\n",
    "    if (arrTestTopic[i] == 'tecnologia'):\n",
    "        TecnTrue += 1\n",
    "    if (arrTestTopic[i] == 'deportes'):\n",
    "        DepTrue += 1\n",
    "    if (arrTestTopic[i] == 'economia'):\n",
    "        EconTrue += 1\n",
    "    if (arrTestTopic[i] == 'cultura'):\n",
    "        CultTrue += 1\n",
    "        \n",
    "print(\"Covid = \", covTrue, \"Tecnología = \", TecnTrue, \"Deportes = \", DepTrue, \"Economia = \", EconTrue, \"Cultura= \", CultTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(arrTest)):\n",
    "    result.append(MAP(arrTest[i], palabrasCovid, AlfabetoCovid, palabrasTecn, AlfabetoTecn, palabrasDep, AlfabetoDep))\n",
    "    \n",
    "resultClass = numpy.asarray(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "resultCov = 0\n",
    "for i in range(len(resultClass)):\n",
    "    #print(resultClass[i], arrTestTopic[i])\n",
    "    if (arrTestTopic[i] == 'cultura' and resultClass[i] == 'cultura'):\n",
    "        resultCov += 1\n",
    "        \n",
    "print(resultCov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
